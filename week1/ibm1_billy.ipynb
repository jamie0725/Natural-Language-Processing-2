{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231164 231164 37 37\n",
      "231164\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from aer import read_naacl_alignments\n",
    "import aer\n",
    "\n",
    "# reading training data\n",
    "training_en = open('./training/hansards.36.2.e').read().splitlines()\n",
    "training_fr = open('./training/hansards.36.2.f').read().splitlines()\n",
    "\n",
    "# reading validation data\n",
    "validation_en = open('./validation/dev.e').read().splitlines()\n",
    "validation_fr = open('./validation/dev.f').read().splitlines()\n",
    "\n",
    "#path for validation goldset\n",
    "path = 'validation/dev.wa.nonullalign'\n",
    "\n",
    "print(len(training_en), len(training_fr), len(validation_en), len(validation_fr))\n",
    "\n",
    "sentence_num = len(training_en)\n",
    "print(sentence_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take all sents and return a (list)set of vocab \n",
    "def vocab_set(list_of_sent):\n",
    "    all_words =[]\n",
    "    for x in list_of_sent:\n",
    "        all_words+=x.split()\n",
    "    return list(set(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a set of all vocabs and return a dictionary for indexing\n",
    "#the position of numpy.array (theta) will be based on this indexing\n",
    "def vocab2dict(vocab_set):\n",
    "    return {vocab_set[x]:x for x in range(len(vocab_set))} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training \n",
    "def training(en_data, fr_data, iteration=10):\n",
    "    #get #total vocab\n",
    "    vocab_set_fr = vocab_set(fr_data)\n",
    "    vocab_set_en = vocab_set(en_data)\n",
    "\n",
    "    fr_vocab_total = len(vocab_set_fr)\n",
    "    en_vocab_total = len(vocab_set_en)\n",
    "    print('Fr word total = %d'%fr_vocab_total)\n",
    "    print('En word total = %d'%en_vocab_total)\n",
    "    \n",
    "    #get word index dicts\n",
    "    fr2index = vocab2dict(vocab_set_fr)\n",
    "    en2index = vocab2dict(vocab_set_en)\n",
    "    \n",
    "    #theta initialization\n",
    "    uniform_distribution = 1/fr_vocab_total\n",
    "\n",
    "    #row = en, column=fr\n",
    "    theta = np.full((en_vocab_total,fr_vocab_total), uniform_distribution, np.float64)    \n",
    "    \n",
    "    for iter_num in range(iteration):\n",
    "        count =np.zeros((en_vocab_total,fr_vocab_total), np.float64)\n",
    "        count_en = np.zeros(en_vocab_total, np.float64)\n",
    "\n",
    "        for n in range(len(en_data)):\n",
    "            en_sent = en_data[n].split()\n",
    "            fr_sent = fr_data[n].split()\n",
    "            #print(fr_sent,en_sent)\n",
    "\n",
    "            for i in range(len(fr_sent)):\n",
    "                fr_word_index = fr2index[fr_sent[i]]\n",
    "                Z = 0\n",
    "\n",
    "                for j in range(len(en_sent)):\n",
    "                    en_word_index = en2index[en_sent[j]]\n",
    "                    Z+= theta[en_word_index][fr_word_index]\n",
    "\n",
    "                for j in range(len(en_sent)):\n",
    "                    en_word_index = en2index[en_sent[j]]\n",
    "                    c = theta[en_word_index][fr_word_index]/Z\n",
    "                    count[en_word_index][fr_word_index] +=c\n",
    "                    count_en[en_word_index]+=c\n",
    "\n",
    "        #for each row (of e word), divide the col values (fr words) by total e \n",
    "        normalized = count/count_en[:,None]\n",
    "        theta=normalized\n",
    "        \n",
    "        eval(fr2index, en2index, theta)\n",
    "        \n",
    "    return 'Done' #theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fr word total = 12235\n",
      "En word total = 9659\n",
      "0.8423040604343721\n",
      "0.858356940509915\n",
      "0.8630783758262512\n",
      "0.8630783758262512\n",
      "0.8668555240793201\n",
      "0.8677998111425873\n",
      "0.8659112370160529\n",
      "0.8611898016997167\n",
      "0.8602455146364495\n",
      "0.858356940509915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training(training_en[:10000], training_fr[:10000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.822982617624\n",
      "0.69883946096\n"
     ]
    }
   ],
   "source": [
    "print(theta[en2index['she']][fr2index['elle']])\n",
    "print(theta[en2index['he']][fr2index['il']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(fr2index, en2index, theta):\n",
    "    predictions = []\n",
    "    for n in range(len(validation_en)):\n",
    "        fr_sent=validation_fr[n].split()\n",
    "        en_sent=validation_en[n].split()\n",
    "        align = set()\n",
    "        \n",
    "        for i in range(len(fr_sent)):\n",
    "            best_p = 0 #en-fr prob\n",
    "            best_j = 0 #eng position\n",
    "            if fr2index.get(fr_sent[i], 'empty')!= 'empty':\n",
    "                fr_ind = fr2index[fr_sent[i]]\n",
    "            \n",
    "                for j in range(len(en_sent)):\n",
    "                    if en2index.get(en_sent[j], 'empty')!= 'empty':\n",
    "                        en_ind = en2index[en_sent[j]]\n",
    "\n",
    "                        if theta[en_ind][fr_ind]>best_p:\n",
    "                            best_p = theta[en_ind][fr_ind]\n",
    "                            best_j = j\n",
    "            align.add((best_j, i+1))   \n",
    "        predictions.append(align)\n",
    "        \n",
    "    gold_sets = read_naacl_alignments(path)\n",
    "\n",
    "\n",
    "    # 3. Compute AER\n",
    "\n",
    "    # first we get an object that manages sufficient statistics \n",
    "    metric = aer.AERSufficientStatistics()\n",
    "    # then we iterate over the corpus \n",
    "    for gold, pred in zip(gold_sets, predictions):\n",
    "        metric.update(sure=gold[0], probable=gold[1], predicted=pred)\n",
    "    # AER\n",
    "    print(metric.aer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(path):\n",
    "    from random import random\n",
    "    # 1. Read in gold alignments\n",
    "    gold_sets = read_naacl_alignments(path)\n",
    "\n",
    "    # 2. Here you would have the predictions of your own algorithm, \n",
    "    #  for the sake of the illustration, I will cheat and make some predictions by corrupting 50% of sure gold alignments\n",
    "    predictions = []\n",
    "    for s, p in gold_sets:\n",
    "        links = set()\n",
    "        for link in s:\n",
    "            if random() < 0.1:\n",
    "                links.add(link)\n",
    "        predictions.append(links)\n",
    "    print(predictions)\n",
    "\n",
    "    # 3. Compute AER\n",
    "\n",
    "    # first we get an object that manages sufficient statistics \n",
    "    metric = aer.AERSufficientStatistics()\n",
    "    # then we iterate over the corpus \n",
    "    for gold, pred in zip(gold_sets, predictions):\n",
    "        metric.update(sure=gold[0], probable=gold[1], predicted=pred)\n",
    "    # AER\n",
    "    print(metric.aer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{(9, 10), (20, 24)}, set(), {(16, 16), (22, 23), (15, 15)}, set(), {(4, 12), (20, 25)}, set(), {(15, 2)}, set(), {(12, 13), (8, 9), (3, 3)}, {(4, 6)}, set(), {(16, 23)}, {(9, 8)}, set(), set(), set(), set(), set(), set(), {(13, 12), (12, 11)}, set(), {(16, 14), (15, 13)}, {(3, 4)}, set(), {(14, 12), (15, 13), (1, 1)}, {(14, 10), (16, 13)}, {(4, 3)}, set(), {(9, 11), (1, 1)}, {(24, 30), (17, 29), (10, 16)}, set(), {(14, 16)}, set(), {(16, 15)}, set(), set(), set()]\n",
      "0.827027027027027\n"
     ]
    }
   ],
   "source": [
    "test('validation/dev.wa.nonullalign')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP2 project 1: Lexical Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import operator\n",
    "import aer\n",
    "from collections import *\n",
    "from decimal import *\n",
    "from aer import read_naacl_alignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of English sentences in training set: 231164\n",
      "Number of French sentences in training set: 231164\n",
      "Number of English sentences in validation set: 37\n",
      "Number of English sentences in validation set: 37\n"
     ]
    }
   ],
   "source": [
    "# reading training data\n",
    "en_train = open('./training/hansards.36.2.e').read().splitlines()\n",
    "fr_train = open('./training/hansards.36.2.f').read().splitlines()\n",
    "\n",
    "# reading validation data\n",
    "en_val = open('./validation/dev.e').read().splitlines()\n",
    "fr_val = open('./validation/dev.f').read().splitlines()\n",
    "\n",
    "# path of gold standrad\n",
    "path = 'validation/dev.wa.nonullalign'\n",
    "\n",
    "print('Number of English sentences in training set:', len(en_train))\n",
    "print('Number of French sentences in training set:', len(fr_train))\n",
    "print('Number of English sentences in validation set:', len(en_val))\n",
    "print('Number of English sentences in validation set:', len(fr_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first English sentence: 36 th Parliament , 2 nd Session \n"
     ]
    }
   ],
   "source": [
    "print('The first English sentence:', en_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first French sentence: 36 e L茅gislature , 2 i猫me Session \n"
     ]
    }
   ],
   "source": [
    "print('The first French sentence:', fr_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theta = np.load('theta_0.npy').item()\n",
    "# theta_10 = np.load('theta_10.npy').item()\n",
    "# count_f_e_0 = np.load('count_f_e_0.npy').item()\n",
    "# count_e_0 = np.load('count_e_0.npy').item()\n",
    "\n",
    "# '''\n",
    "# np.save('theta_0.npy', theta_0) \n",
    "# np.save('count_f_e_0.npy', count_f_e) \n",
    "# np.save('count_e_0.npy', count_e)\n",
    "# np.save('theta_10.npy', theta) \n",
    "# '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(s):\n",
    "    return s.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(en_val)):\n",
    "    en_val[i] = preprocess(en_val[i])\n",
    "    fr_val[i] = preprocess(fr_val[i])\n",
    "for i in range(len(en_train)):\n",
    "    en_train[i] = preprocess(en_train[i])\n",
    "    en_train[i] = ['NULLINDICATOR'] + en_train[i]\n",
    "    fr_train[i] = preprocess(fr_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proccessed example sentence: ['NULLINDICATOR', '36', 'th', 'Parliament', ',', '2', 'nd', 'Session']\n"
     ]
    }
   ],
   "source": [
    "print('Proccessed example sentence:', en_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBM1 - Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_theta(en_train, fr_train):\n",
    "    theta = defaultdict(lambda: defaultdict(lambda: Decimal(0)))\n",
    "    N= len(en_train)\n",
    "    for n in range(N):\n",
    "        cleaned_en_n = en_train[n]\n",
    "        cleaned_fr_n = fr_train[n]     \n",
    "        for j_en_word in cleaned_en_n:\n",
    "            for i_fr_word in cleaned_fr_n:\n",
    "                theta[j_en_word][i_fr_word] = 0      \n",
    "    for en_word in theta.keys():\n",
    "        count = len(theta[en_word])\n",
    "        for fr_word in theta[en_word]:\n",
    "            theta[en_word][fr_word] = Decimal(1 / count)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of English words in the training set: 36636\n"
     ]
    }
   ],
   "source": [
    "theta = init_theta(en_train, fr_train)\n",
    "print('Number of English words in the training set:', len(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBM1 - EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_EM(en_train, fr_train, theta, en_val, fr_val, K):\n",
    "    N = len(en_train)\n",
    "    for k in range(K):\n",
    "        print('Iteration:', k)\n",
    "        count_f_e = defaultdict(lambda: Decimal(0))\n",
    "        total_f = defaultdict(lambda: Decimal(0))\n",
    "        for n in range(N):\n",
    "            cleaned_en_n = en_train[n]\n",
    "            cleaned_fr_n = fr_train[n]\n",
    "            count_e = defaultdict(lambda: Decimal(0))\n",
    "            for j in range(len(cleaned_en_n)):\n",
    "                for i in range(len(cleaned_fr_n)):\n",
    "                    count_e[cleaned_en_n[j]] += Decimal(theta[cleaned_en_n[j]][cleaned_fr_n[i]])\n",
    "            for j in range(len(cleaned_en_n)):\n",
    "                for i in range(len(cleaned_fr_n)):\n",
    "                    f_e = (cleaned_fr_n[i], cleaned_en_n[j])\n",
    "                    count_f_e[f_e] += Decimal(theta[cleaned_en_n[j]][cleaned_fr_n[i]] / count_e[cleaned_en_n[j]])\n",
    "                    total_f[cleaned_fr_n[i]] += Decimal(theta[cleaned_en_n[j]][cleaned_fr_n[i]] / count_e[cleaned_en_n[j]])\n",
    "        for n in range(N):\n",
    "            cleaned_en_n = en_train[n]\n",
    "            cleaned_fr_n = fr_train[n]\n",
    "            for en in cleaned_en_n:\n",
    "                for fr in cleaned_fr_n:\n",
    "                    theta[en][fr] = Decimal(count_f_e[(fr, en)] / total_f[fr])\n",
    "        predictions = []\n",
    "        for s in range(len(en_val)):\n",
    "            alignments=[]\n",
    "            l = len(en_val[s])\n",
    "            m = len(fr_val[s])\n",
    "            for i in range(l):\n",
    "                if i==0: continue\n",
    "                vals=[]\n",
    "                for j in range(m):\n",
    "                    vals.append(theta[en_val[s][i]][fr_val[s][j]])\n",
    "                max_idx = vals.index(max(vals))\n",
    "                alignments.append((i, max_idx+1))\n",
    "            predictions.append(set(alignments))\n",
    "#             vals = []\n",
    "#             for i_en, w_en in enumerate(en_val[s]):\n",
    "#                 if i_en == 0:\n",
    "#                     continue\n",
    "#                 for i_fr, w_fr in enumerate(fr_val[s]):\n",
    "#                     vals.append(theta[w_en][w_fr])\n",
    "#                 max_ = vals.index(max(vals))\n",
    "#                 align.add((i_en, max_+1))\n",
    "#             for i_fr, w_fr in enumerate(fr_val[s]):\n",
    "#                 best_p = 0\n",
    "#                 best_j = 0\n",
    "#                 potential = []\n",
    "#                 for i_en, w_en in enumerate(en_val[s]):\n",
    "#                     if i_en == 0:\n",
    "#                         continue\n",
    "#                     if w_en in theta and w_fr in theta[w_en] and theta[w_en][w_fr] > best_p:\n",
    "#                         best_p = theta[w_en][w_fr]\n",
    "#                         best_j = i_en\n",
    "#                         potential = []\n",
    "#                         potential.append(best_j)\n",
    "#                     if w_en in theta and w_fr in theta[w_en] and abs(theta[w_en][w_fr] - best_p) < 0.05:\n",
    "#                         potential.append(i_en)\n",
    "#                 for candidate in potential:\n",
    "#                     align.add((candidate, i_fr+1))\n",
    "#             predictions.append(align)\n",
    "        gold_sets = read_naacl_alignments(path)\n",
    "        metric = aer.AERSufficientStatistics()\n",
    "        for gold, pred in zip(gold_sets, predictions):\n",
    "            metric.update(sure=gold[0], probable=gold[1], predicted=pred)\n",
    "        # AER\n",
    "        print(metric.aer())\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-2594beb948bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_EM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0men_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfr_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0men_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfr_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-cc113d3002c1>\u001b[0m in \u001b[0;36mtrain_EM\u001b[1;34m(en_train, fr_train, theta, en_val, fr_val, K)\u001b[0m\n\u001b[0;32m     16\u001b[0m                     \u001b[0mf_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcleaned_fr_n\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcleaned_en_n\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mcount_f_e\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf_e\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mDecimal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcleaned_en_n\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcleaned_fr_n\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mcount_e\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcleaned_en_n\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m                     \u001b[0mtotal_f\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcleaned_fr_n\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mDecimal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcleaned_en_n\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcleaned_fr_n\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mcount_e\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcleaned_en_n\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mcleaned_en_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0men_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "theta = train_EM(en_train, fr_train, theta, en_val, fr_val, K=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = theta['read']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_x = sorted(a.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_x[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
